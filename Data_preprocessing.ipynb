{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgS_QwV-GPD8",
        "outputId": "170c1d18-9241-4f1f-afa5-7c986793530a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 最终修复版：大规模HMDA预处理\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "def create_large_hmda_dataset_fixed(file_path, target_samples=100000):\n",
        "    \"\"\"\n",
        "    创建大规模HMDA数据集 - 修复版本\n",
        "    \"\"\"\n",
        "    print(f\"🎯 目标创建 {target_samples:,} 样本的平衡数据集\")\n",
        "    target_per_class = target_samples // 2\n",
        "\n",
        "    try:\n",
        "        # 步骤1: 加载大规模数据\n",
        "        print(f\"\\n=== 步骤1: 大规模数据加载 ===\")\n",
        "\n",
        "        # 读取更多数据以确保有足够的拒绝记录\n",
        "        read_size = 500000  # 读取50万行\n",
        "        print(f\"📖 读取 {read_size:,} 行数据...\")\n",
        "\n",
        "        # 定义需要的列\n",
        "        required_cols = [\n",
        "            'loan_amount', 'income', 'property_value', 'interest_rate',\n",
        "            'loan_type', 'loan_purpose', 'applicant_age', 'state_code', 'county_code',\n",
        "            'derived_race', 'derived_ethnicity', 'derived_sex', 'action_taken'\n",
        "        ]\n",
        "\n",
        "        # 读取数据\n",
        "        df = pd.read_csv(file_path, nrows=read_size, low_memory=False)\n",
        "\n",
        "        # 选择可用的列\n",
        "        available_cols = [col for col in required_cols if col in df.columns]\n",
        "        df = df[available_cols].copy()\n",
        "\n",
        "        print(f\"✅ 数据加载完成: {df.shape}\")\n",
        "        print(f\"使用列: {available_cols}\")\n",
        "\n",
        "        # 步骤2: 温和数据清理\n",
        "        print(f\"\\n=== 步骤2: 温和数据清理 ===\")\n",
        "\n",
        "        # 筛选批准/拒绝\n",
        "        df['action_taken'] = pd.to_numeric(df['action_taken'], errors='coerce')\n",
        "        df_clean = df[df['action_taken'].isin([1, 3])].copy().reset_index(drop=True)\n",
        "\n",
        "        print(f\"筛选批准/拒绝: {len(df)} → {len(df_clean)}\")\n",
        "\n",
        "        # 检查分布\n",
        "        approved_count = (df_clean['action_taken'] == 1).sum()\n",
        "        rejected_count = (df_clean['action_taken'] == 3).sum()\n",
        "        print(f\"批准: {approved_count:,}, 拒绝: {rejected_count:,}\")\n",
        "\n",
        "        # 只做最基本的清理\n",
        "        if 'loan_amount' in df_clean.columns:\n",
        "            df_clean['loan_amount'] = pd.to_numeric(df_clean['loan_amount'], errors='coerce')\n",
        "            df_clean = df_clean.dropna(subset=['loan_amount']).reset_index(drop=True)\n",
        "            df_clean = df_clean[df_clean['loan_amount'] > 1000].reset_index(drop=True)  # 只移除明显错误的\n",
        "\n",
        "        # 重新检查分布\n",
        "        approved_after = (df_clean['action_taken'] == 1).sum()\n",
        "        rejected_after = (df_clean['action_taken'] == 3).sum()\n",
        "        print(f\"清理后 - 批准: {approved_after:,}, 拒绝: {rejected_after:,}\")\n",
        "\n",
        "        # 步骤3: 简化特征工程\n",
        "        print(f\"\\n=== 步骤3: 简化特征工程 ===\")\n",
        "\n",
        "        # 保存敏感属性\n",
        "        sensitive_cols = ['derived_race', 'derived_ethnicity', 'derived_sex']\n",
        "        available_sensitive = [col for col in sensitive_cols if col in df_clean.columns]\n",
        "        sensitive_data = df_clean[available_sensitive].copy()\n",
        "        print(f\"敏感属性: {available_sensitive}\")\n",
        "\n",
        "        # 处理数值特征\n",
        "        numeric_cols = ['loan_amount', 'income', 'property_value', 'interest_rate']\n",
        "        available_numeric = [col for col in numeric_cols if col in df_clean.columns]\n",
        "\n",
        "        feature_dfs = []\n",
        "        feature_names = []\n",
        "\n",
        "        if available_numeric:\n",
        "            # 处理数值特征\n",
        "            numeric_df = df_clean[available_numeric].copy()\n",
        "\n",
        "            for col in available_numeric:\n",
        "                numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')\n",
        "                median_val = numeric_df[col].median()\n",
        "                numeric_df[col] = numeric_df[col].fillna(median_val)\n",
        "\n",
        "                # 移除极端异常值\n",
        "                Q1 = numeric_df[col].quantile(0.01)\n",
        "                Q99 = numeric_df[col].quantile(0.99)\n",
        "                numeric_df[col] = numeric_df[col].clip(Q1, Q99)\n",
        "\n",
        "            # 标准化\n",
        "            scaler = StandardScaler()\n",
        "            numeric_df[available_numeric] = scaler.fit_transform(numeric_df[available_numeric])\n",
        "\n",
        "            feature_dfs.append(numeric_df)\n",
        "            feature_names.extend(available_numeric)\n",
        "            print(f\"✓ 数值特征: {len(available_numeric)} 个\")\n",
        "\n",
        "        # 处理分类特征\n",
        "        categorical_cols = ['loan_type', 'loan_purpose', 'applicant_age', 'state_code']\n",
        "        available_categorical = [col for col in categorical_cols if col in df_clean.columns]\n",
        "\n",
        "        for col in available_categorical:\n",
        "            try:\n",
        "                # 填充缺失值\n",
        "                df_col = df_clean[col].fillna('Unknown')\n",
        "\n",
        "                # 限制类别数量\n",
        "                if df_col.nunique() > 20:\n",
        "                    top_values = df_col.value_counts().head(15).index\n",
        "                    df_col = df_col.apply(lambda x: x if x in top_values else 'Other')\n",
        "\n",
        "                # 独热编码\n",
        "                dummies = pd.get_dummies(df_col, prefix=col, drop_first=True)\n",
        "                feature_dfs.append(dummies)\n",
        "                feature_names.extend(dummies.columns.tolist())\n",
        "                print(f\"  {col}: {len(dummies.columns)} 个特征\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  {col}: 编码失败 - {e}\")\n",
        "\n",
        "        # 合并特征\n",
        "        if not feature_dfs:\n",
        "            print(\"❌ 没有可用特征\")\n",
        "            return None\n",
        "\n",
        "        features_df = pd.concat(feature_dfs, axis=1)\n",
        "        print(f\"✓ 总特征数: {len(feature_names)}\")\n",
        "\n",
        "        # 步骤4: 简单平衡采样\n",
        "        print(f\"\\n=== 步骤4: 简单平衡采样 ===\")\n",
        "\n",
        "        # 创建目标变量\n",
        "        target = (df_clean['action_taken'] == 1).astype(int)\n",
        "\n",
        "        # 分离正负样本\n",
        "        approved_mask = target == 1\n",
        "        rejected_mask = target == 0\n",
        "\n",
        "        approved_features = features_df[approved_mask].reset_index(drop=True)\n",
        "        approved_sensitive = sensitive_data[approved_mask].reset_index(drop=True)\n",
        "\n",
        "        rejected_features = features_df[rejected_mask].reset_index(drop=True)\n",
        "        rejected_sensitive = sensitive_data[rejected_mask].reset_index(drop=True)\n",
        "\n",
        "        print(f\"分离后 - 批准: {len(approved_features):,}, 拒绝: {len(rejected_features):,}\")\n",
        "\n",
        "        # 确定采样数量\n",
        "        actual_per_class = min(target_per_class, len(approved_features), len(rejected_features))\n",
        "        print(f\"每类采样数量: {actual_per_class:,}\")\n",
        "\n",
        "        # 简单随机采样\n",
        "        approved_idx = np.random.choice(len(approved_features), actual_per_class, replace=False)\n",
        "        rejected_idx = np.random.choice(len(rejected_features), actual_per_class, replace=False)\n",
        "\n",
        "        # 采样数据\n",
        "        approved_sampled_features = approved_features.iloc[approved_idx].reset_index(drop=True)\n",
        "        approved_sampled_sensitive = approved_sensitive.iloc[approved_idx].reset_index(drop=True)\n",
        "\n",
        "        rejected_sampled_features = rejected_features.iloc[rejected_idx].reset_index(drop=True)\n",
        "        rejected_sampled_sensitive = rejected_sensitive.iloc[rejected_idx].reset_index(drop=True)\n",
        "\n",
        "        # 合并数据\n",
        "        X = pd.concat([approved_sampled_features, rejected_sampled_features], ignore_index=True)\n",
        "        y = pd.concat([\n",
        "            pd.Series([1] * actual_per_class),\n",
        "            pd.Series([0] * actual_per_class)\n",
        "        ], ignore_index=True)\n",
        "\n",
        "        sensitive_final = pd.concat([\n",
        "            approved_sampled_sensitive,\n",
        "            rejected_sampled_sensitive\n",
        "        ], ignore_index=True)\n",
        "\n",
        "        # 打乱数据\n",
        "        shuffle_idx = np.random.permutation(len(X))\n",
        "        X = X.iloc[shuffle_idx].reset_index(drop=True)\n",
        "        y = y.iloc[shuffle_idx].reset_index(drop=True)\n",
        "        sensitive_final = sensitive_final.iloc[shuffle_idx].reset_index(drop=True)\n",
        "\n",
        "        # 最终结果\n",
        "        total_samples = len(X)\n",
        "        approval_rate = y.mean()\n",
        "\n",
        "        print(f\"\\n✅ 大规模数据集创建成功!\")\n",
        "        print(f\"📊 最终数据集: {total_samples:,} 条记录\")\n",
        "        print(f\"📊 特征数量: {len(feature_names)} 个\")\n",
        "        print(f\"📊 批准率: {approval_rate:.1%}\")\n",
        "\n",
        "        # 代理变量分析\n",
        "        proxy_count = sum(1 for col in feature_names if any(\n",
        "            keyword in col.lower() for keyword in ['state', 'county', 'age', 'loan', 'type']\n",
        "        ))\n",
        "        print(f\"📊 代理变量: ~{proxy_count} 个\")\n",
        "\n",
        "        # 按敏感属性分析\n",
        "        print(f\"\\n📈 按敏感属性的批准率:\")\n",
        "        for col in available_sensitive:\n",
        "            if col in sensitive_final.columns:\n",
        "                print(f\"\\n{col}:\")\n",
        "                for group in sensitive_final[col].value_counts().head(3).index:\n",
        "                    mask = sensitive_final[col] == group\n",
        "                    if mask.sum() > 100:\n",
        "                        rate = y[mask].mean()\n",
        "                        count = mask.sum()\n",
        "                        print(f\"  {group}: {rate:.3f} ({count:,} 样本)\")\n",
        "\n",
        "        # 保存数据\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        try:\n",
        "            X.to_csv(f'hmda_large_final_{timestamp}.csv', index=False)\n",
        "            labels_df = pd.DataFrame({'target': y})\n",
        "            for col in sensitive_final.columns:\n",
        "                labels_df[col] = sensitive_final[col]\n",
        "            labels_df.to_csv(f'hmda_labels_final_{timestamp}.csv', index=False)\n",
        "            print(f\"\\n✅ 数据已保存: hmda_large_final_{timestamp}.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 保存失败: {e}\")\n",
        "\n",
        "        return {\n",
        "            'X': X,\n",
        "            'y': y,\n",
        "            'sensitive_data': sensitive_final,\n",
        "            'feature_names': feature_names,\n",
        "            'stats': {\n",
        "                'total_samples': total_samples,\n",
        "                'n_features': len(feature_names),\n",
        "                'approval_rate': approval_rate,\n",
        "                'proxy_variables': proxy_count\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 处理失败: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def quick_hmda_summary(result):\n",
        "    \"\"\"\n",
        "    快速数据集摘要\n",
        "    \"\"\"\n",
        "    if result is None:\n",
        "        print(\"❌ 没有可用数据\")\n",
        "        return\n",
        "\n",
        "    X, y, sensitive_data = result['X'], result['y'], result['sensitive_data']\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 大规模HMDA数据集创建完成\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"📊 训练特征: {X.shape}\")\n",
        "    print(f\"📊 目标变量: {y.shape}\")\n",
        "    print(f\"📊 敏感属性: {sensitive_data.shape}\")\n",
        "    print(f\"📊 批准率: {y.mean():.1%}\")\n",
        "\n",
        "    # 数据质量检查\n",
        "    print(f\"\\n🔍 数据质量:\")\n",
        "    print(f\"  无缺失值: {X.isnull().sum().sum() == 0}\")\n",
        "    print(f\"  特征范围: {X.std().mean():.3f}\")\n",
        "    print(f\"  标签平衡: {'✓' if abs(y.mean() - 0.5) < 0.01 else '✗'}\")\n",
        "\n",
        "    print(f\"\\n🚀 数据已准备就绪，可以开始神经网络训练!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# 运行函数\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/drive/MyDrive/2023_public_lar_csv.csv\"\n",
        "\n",
        "    # 创建大规模数据集\n",
        "    print(\"开始创建大规模HMDA数据集...\")\n",
        "    result = create_large_hmda_dataset_fixed(file_path, target_samples=100000)\n",
        "\n",
        "    # 显示摘要\n",
        "    quick_hmda_summary(result)\n",
        "\n",
        "    if result:\n",
        "        print(f\"\\n🎯 成功创建了100K样本的平衡数据集!\")\n",
        "        print(f\"现在可以开始建模了...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL1oA98ZGDUR",
        "outputId": "2abc2d54-9b8d-4637-b5a1-cb61b4b5e4cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始创建大规模HMDA数据集...\n",
            "🎯 目标创建 100,000 样本的平衡数据集\n",
            "\n",
            "=== 步骤1: 大规模数据加载 ===\n",
            "📖 读取 500,000 行数据...\n",
            "✅ 数据加载完成: (500000, 13)\n",
            "使用列: ['loan_amount', 'income', 'property_value', 'interest_rate', 'loan_type', 'loan_purpose', 'applicant_age', 'state_code', 'county_code', 'derived_race', 'derived_ethnicity', 'derived_sex', 'action_taken']\n",
            "\n",
            "=== 步骤2: 温和数据清理 ===\n",
            "筛选批准/拒绝: 500000 → 316734\n",
            "批准: 227,827, 拒绝: 88,907\n",
            "清理后 - 批准: 227,827, 拒绝: 88,907\n",
            "\n",
            "=== 步骤3: 简化特征工程 ===\n",
            "敏感属性: ['derived_race', 'derived_ethnicity', 'derived_sex']\n",
            "✓ 数值特征: 4 个\n",
            "  loan_type: 3 个特征\n",
            "  loan_purpose: 5 个特征\n",
            "  applicant_age: 7 个特征\n",
            "  state_code: 15 个特征\n",
            "✓ 总特征数: 34\n",
            "\n",
            "=== 步骤4: 简单平衡采样 ===\n",
            "分离后 - 批准: 227,827, 拒绝: 88,907\n",
            "每类采样数量: 50,000\n",
            "\n",
            "✅ 大规模数据集创建成功!\n",
            "📊 最终数据集: 100,000 条记录\n",
            "📊 特征数量: 34 个\n",
            "📊 批准率: 50.0%\n",
            "📊 代理变量: ~31 个\n",
            "\n",
            "📈 按敏感属性的批准率:\n",
            "\n",
            "derived_race:\n",
            "  White: 0.509 (64,945 样本)\n",
            "  Race Not Available: 0.560 (15,145 样本)\n",
            "  Black or African American: 0.336 (10,991 样本)\n",
            "\n",
            "derived_ethnicity:\n",
            "  Not Hispanic or Latino: 0.500 (72,594 样本)\n",
            "  Ethnicity Not Available: 0.578 (13,595 样本)\n",
            "  Hispanic or Latino: 0.397 (11,321 样本)\n",
            "\n",
            "derived_sex:\n",
            "  Joint: 0.579 (35,168 样本)\n",
            "  Male: 0.440 (33,027 样本)\n",
            "  Female: 0.428 (23,898 样本)\n",
            "\n",
            "✅ 数据已保存: hmda_large_final_20250806_111720.csv\n",
            "\n",
            "============================================================\n",
            "🎉 大规模HMDA数据集创建完成\n",
            "============================================================\n",
            "📊 训练特征: (100000, 34)\n",
            "📊 目标变量: (100000,)\n",
            "📊 敏感属性: (100000, 3)\n",
            "📊 批准率: 50.0%\n",
            "\n",
            "🔍 数据质量:\n",
            "  无缺失值: True\n",
            "  特征范围: 0.323\n",
            "  标签平衡: ✓\n",
            "\n",
            "🚀 数据已准备就绪，可以开始神经网络训练!\n",
            "============================================================\n",
            "\n",
            "🎯 成功创建了100K样本的平衡数据集!\n",
            "现在可以开始建模了...\n"
          ]
        }
      ]
    }
  ]
}