{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgS_QwV-GPD8",
        "outputId": "170c1d18-9241-4f1f-afa5-7c986793530a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æœ€ç»ˆä¿®å¤ç‰ˆï¼šå¤§è§„æ¨¡HMDAé¢„å¤„ç†\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "def create_large_hmda_dataset_fixed(file_path, target_samples=100000):\n",
        "    \"\"\"\n",
        "    åˆ›å»ºå¤§è§„æ¨¡HMDAæ•°æ®é›† - ä¿®å¤ç‰ˆæœ¬\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ¯ ç›®æ ‡åˆ›å»º {target_samples:,} æ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†\")\n",
        "    target_per_class = target_samples // 2\n",
        "\n",
        "    try:\n",
        "        # æ­¥éª¤1: åŠ è½½å¤§è§„æ¨¡æ•°æ®\n",
        "        print(f\"\\n=== æ­¥éª¤1: å¤§è§„æ¨¡æ•°æ®åŠ è½½ ===\")\n",
        "\n",
        "        # è¯»å–æ›´å¤šæ•°æ®ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ‹’ç»è®°å½•\n",
        "        read_size = 500000  # è¯»å–50ä¸‡è¡Œ\n",
        "        print(f\"ğŸ“– è¯»å– {read_size:,} è¡Œæ•°æ®...\")\n",
        "\n",
        "        # å®šä¹‰éœ€è¦çš„åˆ—\n",
        "        required_cols = [\n",
        "            'loan_amount', 'income', 'property_value', 'interest_rate',\n",
        "            'loan_type', 'loan_purpose', 'applicant_age', 'state_code', 'county_code',\n",
        "            'derived_race', 'derived_ethnicity', 'derived_sex', 'action_taken'\n",
        "        ]\n",
        "\n",
        "        # è¯»å–æ•°æ®\n",
        "        df = pd.read_csv(file_path, nrows=read_size, low_memory=False)\n",
        "\n",
        "        # é€‰æ‹©å¯ç”¨çš„åˆ—\n",
        "        available_cols = [col for col in required_cols if col in df.columns]\n",
        "        df = df[available_cols].copy()\n",
        "\n",
        "        print(f\"âœ… æ•°æ®åŠ è½½å®Œæˆ: {df.shape}\")\n",
        "        print(f\"ä½¿ç”¨åˆ—: {available_cols}\")\n",
        "\n",
        "        # æ­¥éª¤2: æ¸©å’Œæ•°æ®æ¸…ç†\n",
        "        print(f\"\\n=== æ­¥éª¤2: æ¸©å’Œæ•°æ®æ¸…ç† ===\")\n",
        "\n",
        "        # ç­›é€‰æ‰¹å‡†/æ‹’ç»\n",
        "        df['action_taken'] = pd.to_numeric(df['action_taken'], errors='coerce')\n",
        "        df_clean = df[df['action_taken'].isin([1, 3])].copy().reset_index(drop=True)\n",
        "\n",
        "        print(f\"ç­›é€‰æ‰¹å‡†/æ‹’ç»: {len(df)} â†’ {len(df_clean)}\")\n",
        "\n",
        "        # æ£€æŸ¥åˆ†å¸ƒ\n",
        "        approved_count = (df_clean['action_taken'] == 1).sum()\n",
        "        rejected_count = (df_clean['action_taken'] == 3).sum()\n",
        "        print(f\"æ‰¹å‡†: {approved_count:,}, æ‹’ç»: {rejected_count:,}\")\n",
        "\n",
        "        # åªåšæœ€åŸºæœ¬çš„æ¸…ç†\n",
        "        if 'loan_amount' in df_clean.columns:\n",
        "            df_clean['loan_amount'] = pd.to_numeric(df_clean['loan_amount'], errors='coerce')\n",
        "            df_clean = df_clean.dropna(subset=['loan_amount']).reset_index(drop=True)\n",
        "            df_clean = df_clean[df_clean['loan_amount'] > 1000].reset_index(drop=True)  # åªç§»é™¤æ˜æ˜¾é”™è¯¯çš„\n",
        "\n",
        "        # é‡æ–°æ£€æŸ¥åˆ†å¸ƒ\n",
        "        approved_after = (df_clean['action_taken'] == 1).sum()\n",
        "        rejected_after = (df_clean['action_taken'] == 3).sum()\n",
        "        print(f\"æ¸…ç†å - æ‰¹å‡†: {approved_after:,}, æ‹’ç»: {rejected_after:,}\")\n",
        "\n",
        "        # æ­¥éª¤3: ç®€åŒ–ç‰¹å¾å·¥ç¨‹\n",
        "        print(f\"\\n=== æ­¥éª¤3: ç®€åŒ–ç‰¹å¾å·¥ç¨‹ ===\")\n",
        "\n",
        "        # ä¿å­˜æ•æ„Ÿå±æ€§\n",
        "        sensitive_cols = ['derived_race', 'derived_ethnicity', 'derived_sex']\n",
        "        available_sensitive = [col for col in sensitive_cols if col in df_clean.columns]\n",
        "        sensitive_data = df_clean[available_sensitive].copy()\n",
        "        print(f\"æ•æ„Ÿå±æ€§: {available_sensitive}\")\n",
        "\n",
        "        # å¤„ç†æ•°å€¼ç‰¹å¾\n",
        "        numeric_cols = ['loan_amount', 'income', 'property_value', 'interest_rate']\n",
        "        available_numeric = [col for col in numeric_cols if col in df_clean.columns]\n",
        "\n",
        "        feature_dfs = []\n",
        "        feature_names = []\n",
        "\n",
        "        if available_numeric:\n",
        "            # å¤„ç†æ•°å€¼ç‰¹å¾\n",
        "            numeric_df = df_clean[available_numeric].copy()\n",
        "\n",
        "            for col in available_numeric:\n",
        "                numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')\n",
        "                median_val = numeric_df[col].median()\n",
        "                numeric_df[col] = numeric_df[col].fillna(median_val)\n",
        "\n",
        "                # ç§»é™¤æç«¯å¼‚å¸¸å€¼\n",
        "                Q1 = numeric_df[col].quantile(0.01)\n",
        "                Q99 = numeric_df[col].quantile(0.99)\n",
        "                numeric_df[col] = numeric_df[col].clip(Q1, Q99)\n",
        "\n",
        "            # æ ‡å‡†åŒ–\n",
        "            scaler = StandardScaler()\n",
        "            numeric_df[available_numeric] = scaler.fit_transform(numeric_df[available_numeric])\n",
        "\n",
        "            feature_dfs.append(numeric_df)\n",
        "            feature_names.extend(available_numeric)\n",
        "            print(f\"âœ“ æ•°å€¼ç‰¹å¾: {len(available_numeric)} ä¸ª\")\n",
        "\n",
        "        # å¤„ç†åˆ†ç±»ç‰¹å¾\n",
        "        categorical_cols = ['loan_type', 'loan_purpose', 'applicant_age', 'state_code']\n",
        "        available_categorical = [col for col in categorical_cols if col in df_clean.columns]\n",
        "\n",
        "        for col in available_categorical:\n",
        "            try:\n",
        "                # å¡«å……ç¼ºå¤±å€¼\n",
        "                df_col = df_clean[col].fillna('Unknown')\n",
        "\n",
        "                # é™åˆ¶ç±»åˆ«æ•°é‡\n",
        "                if df_col.nunique() > 20:\n",
        "                    top_values = df_col.value_counts().head(15).index\n",
        "                    df_col = df_col.apply(lambda x: x if x in top_values else 'Other')\n",
        "\n",
        "                # ç‹¬çƒ­ç¼–ç \n",
        "                dummies = pd.get_dummies(df_col, prefix=col, drop_first=True)\n",
        "                feature_dfs.append(dummies)\n",
        "                feature_names.extend(dummies.columns.tolist())\n",
        "                print(f\"  {col}: {len(dummies.columns)} ä¸ªç‰¹å¾\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  {col}: ç¼–ç å¤±è´¥ - {e}\")\n",
        "\n",
        "        # åˆå¹¶ç‰¹å¾\n",
        "        if not feature_dfs:\n",
        "            print(\"âŒ æ²¡æœ‰å¯ç”¨ç‰¹å¾\")\n",
        "            return None\n",
        "\n",
        "        features_df = pd.concat(feature_dfs, axis=1)\n",
        "        print(f\"âœ“ æ€»ç‰¹å¾æ•°: {len(feature_names)}\")\n",
        "\n",
        "        # æ­¥éª¤4: ç®€å•å¹³è¡¡é‡‡æ ·\n",
        "        print(f\"\\n=== æ­¥éª¤4: ç®€å•å¹³è¡¡é‡‡æ · ===\")\n",
        "\n",
        "        # åˆ›å»ºç›®æ ‡å˜é‡\n",
        "        target = (df_clean['action_taken'] == 1).astype(int)\n",
        "\n",
        "        # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬\n",
        "        approved_mask = target == 1\n",
        "        rejected_mask = target == 0\n",
        "\n",
        "        approved_features = features_df[approved_mask].reset_index(drop=True)\n",
        "        approved_sensitive = sensitive_data[approved_mask].reset_index(drop=True)\n",
        "\n",
        "        rejected_features = features_df[rejected_mask].reset_index(drop=True)\n",
        "        rejected_sensitive = sensitive_data[rejected_mask].reset_index(drop=True)\n",
        "\n",
        "        print(f\"åˆ†ç¦»å - æ‰¹å‡†: {len(approved_features):,}, æ‹’ç»: {len(rejected_features):,}\")\n",
        "\n",
        "        # ç¡®å®šé‡‡æ ·æ•°é‡\n",
        "        actual_per_class = min(target_per_class, len(approved_features), len(rejected_features))\n",
        "        print(f\"æ¯ç±»é‡‡æ ·æ•°é‡: {actual_per_class:,}\")\n",
        "\n",
        "        # ç®€å•éšæœºé‡‡æ ·\n",
        "        approved_idx = np.random.choice(len(approved_features), actual_per_class, replace=False)\n",
        "        rejected_idx = np.random.choice(len(rejected_features), actual_per_class, replace=False)\n",
        "\n",
        "        # é‡‡æ ·æ•°æ®\n",
        "        approved_sampled_features = approved_features.iloc[approved_idx].reset_index(drop=True)\n",
        "        approved_sampled_sensitive = approved_sensitive.iloc[approved_idx].reset_index(drop=True)\n",
        "\n",
        "        rejected_sampled_features = rejected_features.iloc[rejected_idx].reset_index(drop=True)\n",
        "        rejected_sampled_sensitive = rejected_sensitive.iloc[rejected_idx].reset_index(drop=True)\n",
        "\n",
        "        # åˆå¹¶æ•°æ®\n",
        "        X = pd.concat([approved_sampled_features, rejected_sampled_features], ignore_index=True)\n",
        "        y = pd.concat([\n",
        "            pd.Series([1] * actual_per_class),\n",
        "            pd.Series([0] * actual_per_class)\n",
        "        ], ignore_index=True)\n",
        "\n",
        "        sensitive_final = pd.concat([\n",
        "            approved_sampled_sensitive,\n",
        "            rejected_sampled_sensitive\n",
        "        ], ignore_index=True)\n",
        "\n",
        "        # æ‰“ä¹±æ•°æ®\n",
        "        shuffle_idx = np.random.permutation(len(X))\n",
        "        X = X.iloc[shuffle_idx].reset_index(drop=True)\n",
        "        y = y.iloc[shuffle_idx].reset_index(drop=True)\n",
        "        sensitive_final = sensitive_final.iloc[shuffle_idx].reset_index(drop=True)\n",
        "\n",
        "        # æœ€ç»ˆç»“æœ\n",
        "        total_samples = len(X)\n",
        "        approval_rate = y.mean()\n",
        "\n",
        "        print(f\"\\nâœ… å¤§è§„æ¨¡æ•°æ®é›†åˆ›å»ºæˆåŠŸ!\")\n",
        "        print(f\"ğŸ“Š æœ€ç»ˆæ•°æ®é›†: {total_samples:,} æ¡è®°å½•\")\n",
        "        print(f\"ğŸ“Š ç‰¹å¾æ•°é‡: {len(feature_names)} ä¸ª\")\n",
        "        print(f\"ğŸ“Š æ‰¹å‡†ç‡: {approval_rate:.1%}\")\n",
        "\n",
        "        # ä»£ç†å˜é‡åˆ†æ\n",
        "        proxy_count = sum(1 for col in feature_names if any(\n",
        "            keyword in col.lower() for keyword in ['state', 'county', 'age', 'loan', 'type']\n",
        "        ))\n",
        "        print(f\"ğŸ“Š ä»£ç†å˜é‡: ~{proxy_count} ä¸ª\")\n",
        "\n",
        "        # æŒ‰æ•æ„Ÿå±æ€§åˆ†æ\n",
        "        print(f\"\\nğŸ“ˆ æŒ‰æ•æ„Ÿå±æ€§çš„æ‰¹å‡†ç‡:\")\n",
        "        for col in available_sensitive:\n",
        "            if col in sensitive_final.columns:\n",
        "                print(f\"\\n{col}:\")\n",
        "                for group in sensitive_final[col].value_counts().head(3).index:\n",
        "                    mask = sensitive_final[col] == group\n",
        "                    if mask.sum() > 100:\n",
        "                        rate = y[mask].mean()\n",
        "                        count = mask.sum()\n",
        "                        print(f\"  {group}: {rate:.3f} ({count:,} æ ·æœ¬)\")\n",
        "\n",
        "        # ä¿å­˜æ•°æ®\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        try:\n",
        "            X.to_csv(f'hmda_large_final_{timestamp}.csv', index=False)\n",
        "            labels_df = pd.DataFrame({'target': y})\n",
        "            for col in sensitive_final.columns:\n",
        "                labels_df[col] = sensitive_final[col]\n",
        "            labels_df.to_csv(f'hmda_labels_final_{timestamp}.csv', index=False)\n",
        "            print(f\"\\nâœ… æ•°æ®å·²ä¿å­˜: hmda_large_final_{timestamp}.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ä¿å­˜å¤±è´¥: {e}\")\n",
        "\n",
        "        return {\n",
        "            'X': X,\n",
        "            'y': y,\n",
        "            'sensitive_data': sensitive_final,\n",
        "            'feature_names': feature_names,\n",
        "            'stats': {\n",
        "                'total_samples': total_samples,\n",
        "                'n_features': len(feature_names),\n",
        "                'approval_rate': approval_rate,\n",
        "                'proxy_variables': proxy_count\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å¤„ç†å¤±è´¥: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def quick_hmda_summary(result):\n",
        "    \"\"\"\n",
        "    å¿«é€Ÿæ•°æ®é›†æ‘˜è¦\n",
        "    \"\"\"\n",
        "    if result is None:\n",
        "        print(\"âŒ æ²¡æœ‰å¯ç”¨æ•°æ®\")\n",
        "        return\n",
        "\n",
        "    X, y, sensitive_data = result['X'], result['y'], result['sensitive_data']\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ‰ å¤§è§„æ¨¡HMDAæ•°æ®é›†åˆ›å»ºå®Œæˆ\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"ğŸ“Š è®­ç»ƒç‰¹å¾: {X.shape}\")\n",
        "    print(f\"ğŸ“Š ç›®æ ‡å˜é‡: {y.shape}\")\n",
        "    print(f\"ğŸ“Š æ•æ„Ÿå±æ€§: {sensitive_data.shape}\")\n",
        "    print(f\"ğŸ“Š æ‰¹å‡†ç‡: {y.mean():.1%}\")\n",
        "\n",
        "    # æ•°æ®è´¨é‡æ£€æŸ¥\n",
        "    print(f\"\\nğŸ” æ•°æ®è´¨é‡:\")\n",
        "    print(f\"  æ— ç¼ºå¤±å€¼: {X.isnull().sum().sum() == 0}\")\n",
        "    print(f\"  ç‰¹å¾èŒƒå›´: {X.std().mean():.3f}\")\n",
        "    print(f\"  æ ‡ç­¾å¹³è¡¡: {'âœ“' if abs(y.mean() - 0.5) < 0.01 else 'âœ—'}\")\n",
        "\n",
        "    print(f\"\\nğŸš€ æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ç¥ç»ç½‘ç»œè®­ç»ƒ!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# è¿è¡Œå‡½æ•°\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/drive/MyDrive/2023_public_lar_csv.csv\"\n",
        "\n",
        "    # åˆ›å»ºå¤§è§„æ¨¡æ•°æ®é›†\n",
        "    print(\"å¼€å§‹åˆ›å»ºå¤§è§„æ¨¡HMDAæ•°æ®é›†...\")\n",
        "    result = create_large_hmda_dataset_fixed(file_path, target_samples=100000)\n",
        "\n",
        "    # æ˜¾ç¤ºæ‘˜è¦\n",
        "    quick_hmda_summary(result)\n",
        "\n",
        "    if result:\n",
        "        print(f\"\\nğŸ¯ æˆåŠŸåˆ›å»ºäº†100Kæ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†!\")\n",
        "        print(f\"ç°åœ¨å¯ä»¥å¼€å§‹å»ºæ¨¡äº†...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL1oA98ZGDUR",
        "outputId": "2abc2d54-9b8d-4637-b5a1-cb61b4b5e4cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å¼€å§‹åˆ›å»ºå¤§è§„æ¨¡HMDAæ•°æ®é›†...\n",
            "ğŸ¯ ç›®æ ‡åˆ›å»º 100,000 æ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†\n",
            "\n",
            "=== æ­¥éª¤1: å¤§è§„æ¨¡æ•°æ®åŠ è½½ ===\n",
            "ğŸ“– è¯»å– 500,000 è¡Œæ•°æ®...\n",
            "âœ… æ•°æ®åŠ è½½å®Œæˆ: (500000, 13)\n",
            "ä½¿ç”¨åˆ—: ['loan_amount', 'income', 'property_value', 'interest_rate', 'loan_type', 'loan_purpose', 'applicant_age', 'state_code', 'county_code', 'derived_race', 'derived_ethnicity', 'derived_sex', 'action_taken']\n",
            "\n",
            "=== æ­¥éª¤2: æ¸©å’Œæ•°æ®æ¸…ç† ===\n",
            "ç­›é€‰æ‰¹å‡†/æ‹’ç»: 500000 â†’ 316734\n",
            "æ‰¹å‡†: 227,827, æ‹’ç»: 88,907\n",
            "æ¸…ç†å - æ‰¹å‡†: 227,827, æ‹’ç»: 88,907\n",
            "\n",
            "=== æ­¥éª¤3: ç®€åŒ–ç‰¹å¾å·¥ç¨‹ ===\n",
            "æ•æ„Ÿå±æ€§: ['derived_race', 'derived_ethnicity', 'derived_sex']\n",
            "âœ“ æ•°å€¼ç‰¹å¾: 4 ä¸ª\n",
            "  loan_type: 3 ä¸ªç‰¹å¾\n",
            "  loan_purpose: 5 ä¸ªç‰¹å¾\n",
            "  applicant_age: 7 ä¸ªç‰¹å¾\n",
            "  state_code: 15 ä¸ªç‰¹å¾\n",
            "âœ“ æ€»ç‰¹å¾æ•°: 34\n",
            "\n",
            "=== æ­¥éª¤4: ç®€å•å¹³è¡¡é‡‡æ · ===\n",
            "åˆ†ç¦»å - æ‰¹å‡†: 227,827, æ‹’ç»: 88,907\n",
            "æ¯ç±»é‡‡æ ·æ•°é‡: 50,000\n",
            "\n",
            "âœ… å¤§è§„æ¨¡æ•°æ®é›†åˆ›å»ºæˆåŠŸ!\n",
            "ğŸ“Š æœ€ç»ˆæ•°æ®é›†: 100,000 æ¡è®°å½•\n",
            "ğŸ“Š ç‰¹å¾æ•°é‡: 34 ä¸ª\n",
            "ğŸ“Š æ‰¹å‡†ç‡: 50.0%\n",
            "ğŸ“Š ä»£ç†å˜é‡: ~31 ä¸ª\n",
            "\n",
            "ğŸ“ˆ æŒ‰æ•æ„Ÿå±æ€§çš„æ‰¹å‡†ç‡:\n",
            "\n",
            "derived_race:\n",
            "  White: 0.509 (64,945 æ ·æœ¬)\n",
            "  Race Not Available: 0.560 (15,145 æ ·æœ¬)\n",
            "  Black or African American: 0.336 (10,991 æ ·æœ¬)\n",
            "\n",
            "derived_ethnicity:\n",
            "  Not Hispanic or Latino: 0.500 (72,594 æ ·æœ¬)\n",
            "  Ethnicity Not Available: 0.578 (13,595 æ ·æœ¬)\n",
            "  Hispanic or Latino: 0.397 (11,321 æ ·æœ¬)\n",
            "\n",
            "derived_sex:\n",
            "  Joint: 0.579 (35,168 æ ·æœ¬)\n",
            "  Male: 0.440 (33,027 æ ·æœ¬)\n",
            "  Female: 0.428 (23,898 æ ·æœ¬)\n",
            "\n",
            "âœ… æ•°æ®å·²ä¿å­˜: hmda_large_final_20250806_111720.csv\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ å¤§è§„æ¨¡HMDAæ•°æ®é›†åˆ›å»ºå®Œæˆ\n",
            "============================================================\n",
            "ğŸ“Š è®­ç»ƒç‰¹å¾: (100000, 34)\n",
            "ğŸ“Š ç›®æ ‡å˜é‡: (100000,)\n",
            "ğŸ“Š æ•æ„Ÿå±æ€§: (100000, 3)\n",
            "ğŸ“Š æ‰¹å‡†ç‡: 50.0%\n",
            "\n",
            "ğŸ” æ•°æ®è´¨é‡:\n",
            "  æ— ç¼ºå¤±å€¼: True\n",
            "  ç‰¹å¾èŒƒå›´: 0.323\n",
            "  æ ‡ç­¾å¹³è¡¡: âœ“\n",
            "\n",
            "ğŸš€ æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ç¥ç»ç½‘ç»œè®­ç»ƒ!\n",
            "============================================================\n",
            "\n",
            "ğŸ¯ æˆåŠŸåˆ›å»ºäº†100Kæ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†!\n",
            "ç°åœ¨å¯ä»¥å¼€å§‹å»ºæ¨¡äº†...\n"
          ]
        }
      ]
    }
  ]
}